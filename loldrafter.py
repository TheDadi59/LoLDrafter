# -*- coding: utf-8 -*-
"""LoLDrafter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1op75iZCYgXrA79TKGpxlEGhx2v8WF2Fm
"""

import sklearn
import pandas as pd
import numpy as np

pd.set_option('display.max_columns', None)

# Montage du Drive
#from google.colab import drive
#drive.mount('/content/drive', force_remount=True)

#import tensorflow as tf
#tf.__version__

# Get the data
#data_clean = "/content/drive/MyDrive/LoLDrafter/data/Match_data11 _clean.csv"
data_clean_path = "/content/drive/MyDrive/LoLDrafter/data/match_data.csv"

columns = ['WIN'] + [f'ID_{k}' for k in range(1, 11)] + [f'ROLE_{k}' for k in range(1, 11)]
df = pd.read_csv(data_clean_path, sep=';', names=columns)
df = df.replace(' ', pd.NA).dropna(axis=0) # Filter out the rows that contain " "
df['WIN'] = df['WIN'].astype(int)

df.head()

nb_champions = 166

# Encode the input champion ids as one-hot for every champion
def encode_champion(champion_id):
  correspondances = [
    266, 103, 84, 166, 12, 32, 34, 1, 523, 22,
    136, 268, 432, 200, 53, 63, 201, 233, 51, 164,
    69, 31, 42, 122, 131, 119, 36, 245, 60, 28, 81,
    9, 114, 105, 3, 41, 86, 150, 79, 104, 887, 120,
    74, 910, 420, 39, 427, 40, 59, 24, 126, 202, 222,
    145, 429, 43, 30, 38, 55, 10, 141, 85, 121, 203,
    240, 96, 897, 7, 64, 89, 876, 127, 236, 117, 99,
    54, 90, 57, 11, 902, 21, 62, 82, 25, 950, 267,
    75, 111, 518, 76, 895, 56, 20, 2, 61, 516, 80,
    78, 555, 246, 133, 497, 33, 421, 526, 888, 58, 107,
    92, 68, 13, 360, 113, 235, 147, 875, 35, 98, 102,
    27, 14, 15, 72, 37, 16, 50, 517, 134, 223, 163,
    91, 44, 17, 412, 18, 48, 23, 4, 29, 77, 6, 110,
    67, 45, 161, 711, 254, 234, 112, 8, 106, 19, 498,
    101, 5, 157, 777, 83, 350, 154, 238, 221, 115, 26,
    142, 143
  ]
  encoding = np.zeros(nb_champions)
  index = correspondances.index(champion_id)
  encoding[index] = 1
  return encoding

def encode_champions(champions_ids):
  """
  Encodes the given list of champions ids into an input vector of size nb_champions * 10 (number of players in each LoL game)
  """
  encoding = np.array([])
  for id in champions_ids:
    encoding = np.concatenate((encoding, encode_champion(id)))
  return encoding

def encode_role(role):
  roles = {
      "TOP": 0,
      "JUNGLE": 1,
      "MIDDLE": 2,
      "BOTTOM": 3,
      "UTILITY": 4
  }
  role = role.strip()
  encoding = np.zeros(len(roles))
  encoding[roles[role]] = 1
  return encoding

def encode_roles(roles):
  encoding = np.array([])
  for role in roles:
    encoding = np.concatenate((encoding, encode_role(role)))
  return encoding

ids = [266, 76, 895, 56, 20, 2, 61, 516, 80, 78]
encode_champions(ids)
roles = ["TOP",	"JUNGLE",	"MIDDLE",	"BOTTOM",	"UTILITY",	"TOP",	"JUNGLE",	"MIDDLE",	"BOTTOM",	"UTILITY"]
encode_roles(roles)

def augment_row(row):
  row = [int(champion_id) for champion_id in row[:10]] + [role for role in row[10:]]
  version_1 = np.concatenate((encode_champions(row[:10]), encode_roles(row[10:])))
  teams_reversed = row[5:10] + row[:5]
  roles_reversed = row[15:] + row[10:15]
  version_2 = np.concatenate((encode_champions(teams_reversed), encode_roles(roles_reversed)))
  return np.array(version_1), np.array(version_2)

def augment_data(X, y):
  nb_features = 1710
  new_X = np.array([augment_row(row) for row in X]).reshape((-1, nb_features))
  new_y = np.array([(label, 1-label) for label in y]).flatten()
  return new_X, new_y

# Split training, validation and test data
from sklearn.model_selection import train_test_split

X = df.drop(["WIN"], axis=1)
X = X.values

Y = df['WIN']
X, Y = augment_data(X, Y)
X.shape, Y.shape

X_train, X_, y_train, y_ = train_test_split(X, Y, test_size=0.15, stratify=Y)
X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5, stratify=y_)

y_train.shape, y_val.shape, y_test.shape, X_train.shape, X_val.shape, X_test.shape

from sklearn.metrics import accuracy_score, classification_report

def fit_eval_model(model, X_train, y_train, X_val, y_val,):
  model.fit(X_train, y_train)

  train_predictions = model.predict(X_train)
  train_accuracy = accuracy_score(y_train, train_predictions)

  eval_predictions = model.predict(X_val)
  eval_accuracy = accuracy_score(y_val, eval_predictions)

  print("Training Accuracy:", train_accuracy)
  print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

  print("Evaluation Accuracy:", eval_accuracy)
  print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))
  return train_accuracy, eval_accuracy

# First model: SVC
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

lambdas = [1 ** -i for i in range(5)]
degrees = list(range(1, 5))

train_accuracies, eval_accuracies = [[]] * len(lambdas), [[]] * len(lambdas)
"""
for idx_l, _lambda in enumerate(lambdas):
  for degree in  tqdm(degrees):
    model_svc = SVC(kernel='poly', C=_lambda, degree=degree) # With fewer examples, a linear kernel works best (0.6 val accuracy vs)
    model_svc.fit(X_train, y_train)

    train_predictions = model_svc.predict(X_train)
    train_accuracy = accuracy_score(y_train, train_predictions)
    train_accuracies[idx_l].append(train_accuracy)

    print("Training Accuracy:", train_accuracy)
    print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

    eval_predictions = model_svc.predict(X_val)
    eval_accuracy = accuracy_score(y_val, eval_predictions)
    eval_accuracies[idx_l].append(eval_accuracy)

    print("Evaluation Accuracy:", eval_accuracy)
    print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

train_accuracies, eval_accuracies"""
svc_model = SVC(kernel='poly')
svc_model.fit(X_train, y_train)

train_predictions = svc_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

eval_predictions = svc_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Second model: Logistic regression
from sklearn.linear_model import LogisticRegression

logistic_model = LogisticRegression()
logistic_model.fit(X_train, y_train)

train_predictions = logistic_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

eval_predictions = logistic_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Third model: Decision trees
from sklearn.tree import DecisionTreeClassifier

tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)

train_predictions = tree_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

eval_predictions = tree_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Fourth model: Random forests
from sklearn.ensemble import RandomForestClassifier

forest_model = RandomForestClassifier()
forest_model.fit(X_train, y_train)

train_predictions = forest_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

eval_predictions = forest_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Fifth model: XGBoost
from xgboost import XGBClassifier

xgb_model = XGBClassifier()
xgb_model.fit(X_train, y_train)

train_predictions = xgb_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

eval_predictions = xgb_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Sixth model: Naive Bayes
from sklearn.naive_bayes import BernoulliNB

nb_model = BernoulliNB()
nb_model.fit(X_train, y_train)

train_predictions = nb_model.predict(X_train)
train_accuracy = accuracy_score(y_train, train_predictions)

eval_predictions = nb_model.predict(X_val)
eval_accuracy = accuracy_score(y_val, eval_predictions)

print("Training Accuracy:", train_accuracy)
print("Classification Report (Training):\n", classification_report(y_train, train_predictions))

print("Evaluation Accuracy:", eval_accuracy)
print("Classification Report (Evaluation):\n", classification_report(y_val, eval_predictions))

# Seventh model: MLP
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

nb_champions = 166
nb_roles = 5
nb_players = 10
input_dim = (nb_champions + nb_roles) * nb_players

# A simple Multi-Layer Perceptron model with four layers: input layer with 64 ReLU units, and two hidden layers with ReLU units (32 and 16 respectively).
# Output layer uses a sigmoid activation function since we're doing binary classification
mlp_model = Sequential([
    Dense(units=64, activation='relu', input_dim=input_dim),
    Dense(units=32, activation='relu'),
    Dense(units=16, activation='relu'),
    Dense(units=1, activation='sigmoid')
])

mlp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
X_train, X_val = np.array(X_train), np.array(X_val)
history = mlp_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))

mlp_model.save('mlp-model.keras')

loss_mlp, accuracy_mlp = mlp_model.evaluate(X_test, y_test)
loss_mlp, accuracy_mlp

# Eighth model: RNN using LSTM units
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM

nb_champions = 166
nb_roles = 5
nb_players = 10
input_dim = (nb_champions + nb_roles) * nb_players

embedding_dim=256
X_train_rnn, X_val_rnn, X_test_rnn = X_train.reshape((X_train.shape[0], 1, input_dim)), X_val.reshape((X_val.shape[0], 1, input_dim)), X_test.reshape((X_test.shape[0], 1, input_dim))
y_train_rnn, y_val_rnn, y_test_rnn = y_train.reshape((-1, 1)), y_val.reshape((-1, 1)), y_test.reshape((-1, 1))
print(X_train_rnn.shape)
print(X_train_rnn)

rnn_model = Sequential([
    LSTM(units=40, input_shape=(1, input_dim), return_sequences=True),
    LSTM(units=20, return_sequences=True),
    Dense(units=1, activation='sigmoid')
])

rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

rnn_history = rnn_model.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn))

rnn_model.save('rnn-model.keras')

loss_rnn, accuracy_rnn = rnn_model.evaluate(X_test_rnn, y_test_rnn)
loss_rnn, accuracy_rnn

# Ninth model: RNNs using LSTM layers with Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout

nb_champions = 166
nb_roles = 5
nb_players = 10
input_dim = (nb_champions + nb_roles) * nb_players

X_train_rnn, X_val_rnn, X_test_rnn = X_train.reshape((X_train.shape[0], 1, input_dim)), X_val.reshape((X_val.shape[0], 1, input_dim)), X_test.reshape((X_test.shape[0], 1, input_dim))
y_train_rnn, y_val_rnn, y_test_rnn = y_train.reshape((-1, 1)), y_val.reshape((-1, 1)), y_test.reshape((-1, 1))

# A second RNN model using LSTM layers with dropout to see if it reduces variance
rnn_model2 = Sequential([
    LSTM(units=40, input_shape=(1, input_dim),return_sequences=True),
    Dropout(0.2),
    LSTM(units=20, return_sequences=True),
    Dropout(0.2),
    Dense(units=1, activation='sigmoid')
])

rnn_model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

rnn_history2 = rnn_model2.fit(X_train_rnn, y_train_rnn, epochs=50, batch_size=32, validation_data=(X_val_rnn, y_val_rnn))

rnn_model2.save('rnn-model2.keras')

loss2, acc2 = rnn_model2.evaluate(X_test_rnn, y_test_rnn)
loss2, acc2
x = np.array()
rnn_model2.predict

import numpy as np

def make_predictions(X_data):
    roles = ["TOP",	"JUNGLE",	"MIDDLE",	"BOTTOM",	"UTILITY",	"TOP",	"JUNGLE",	"MIDDLE",	"BOTTOM",	"UTILITY"]
    champion_encoding = encode_champions(X_data)
    roles_encoding = encode_roles(roles)

    # Concatenate the two encodings along the second axis (axis=1)
    X = np.concatenate((champion_encoding, roles_encoding)).reshape((1, -1))

    # Make predictions using the trained model
    predictions = mlp_model.predict(X)
    threshold=0.5
    # If your output layer uses sigmoid activation for binary classification, you might want to threshold the predictions
    binary_predictions = (predictions > threshold).astype(int)

    return binary_predictions

# Example usage:
predictions = make_predictions()
print(predictions)

predictions2 = make_predictions()
print(predictions2)